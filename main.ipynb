{
   "cells": [
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# MIMIC_Sepsis\n",
            "\n",
            "## 1 Preparation\n",
            "\n",
            "To run this document the following requirements must be satisfied:\n",
            "\n",
            "- Implement the database mimic in **PostgreSQL** and start it. The instruction can be seen [here](https://github.com/MIT-LCP/mimic-code/tree/main/mimic-iv/buildmimic/postgres). (The name of this environment should be **mimiciv**)\n",
            "- generate useful abstractions of raw MIMIC-IV data. The instruction be seen [here](https://github.com/MIT-LCP/mimic-code/tree/main/mimic-iv/concepts_postgres) \n",
            "\n",
            "\n",
            "\n",
            "To install all the libraries, run:\n",
            "```\n",
            "pip install psycopg2 csv pandas sklearn\n",
            "```\n",
            "\n",
            "\n",
            "After all the preparation is done, run the following cell to set the connection to the database.\n",
            " "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [],
         "source": [
            "import psycopg2\n",
            "from psycopg2 import sql\n",
            "import csv\n",
            "import pandas as pd\n",
            "import os\n",
            "import shutil\n",
            "import csv\n",
            "from datetime import timedelta\n",
            "from sklearn.impute import KNNImputer\n",
            "from sklearn.neighbors import KNeighborsRegressor\n",
            "\n",
            "# implement the username, password and database name\n",
            "conn = psycopg2.connect(host='', user='', password='', database='mimiciv')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 2 Extract selected data from the original database \n",
            "\n",
            "According to the paper we need to extract the **state space** and **action space** respectively from the mimiciv database. The table **mimic4 itemid.csv** lists all the items required.\n",
            "\n",
            "* Here the *IV fluid bolus* of the action space is removed as no data could be found.\n",
            "\n",
            "**Uncomment the following line if you first time run the code**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "# # uncomment the following line if you first time run the code\n",
            "\n",
            "# # Read the SQL file\n",
            "\n",
            "# try:\n",
            "#     with open('sql_file/selection_patients_cohort.sql', 'r') as file0:\n",
            "#         sql_script_selection_patients_cohort = file0.read()\n",
            "\n",
            "#     with open('sql_file/action_from_inputevents.sql', 'r') as file1:\n",
            "#         sql_script_action = file1.read()\n",
            "        \n",
            "#     with open('sql_file/chartevents_dataneeded.sql', 'r') as file2:\n",
            "#         sql_script_state = file2.read()\n",
            "\n",
            "#     # Execute the SQL script\n",
            "#     cursor = conn.cursor()\n",
            "    \n",
            "#     # cursor.execute(sql.SQL(sql_script_selection_patients_cohort))\n",
            "#     # cursor.execute(sql.SQL(sql_script_action))\n",
            "#     # cursor.execute(sql.SQL(sql_script_state))\n",
            "\n",
            "#     conn.commit()\n",
            "#     cursor.close()\n",
            "    \n",
            "    \n",
            "# except (Exception, psycopg2.DatabaseError) as error:\n",
            "#     print(\"Error executing SQL statement:\", error)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Number of stay_ids: 6669\n"
               ]
            }
         ],
         "source": [
            "with conn.cursor() as cursor:\n",
            "    command = \"SELECT distinct stay_id FROM mimiciv_derived.sepsis_patients_cohort ;\"\n",
            "    cursor.execute(command)   \n",
            "    result = cursor.fetchall()\n",
            "    stay_ids= [row[0] for row in result]\n",
            "    num_stay_ids = len(stay_ids)\n",
            "    print('Number of stay_ids: ' + str(num_stay_ids))\n",
            "    cursor.close()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 3 Data transfor"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 3.1 Data transfer of State Space\n",
            "We transfer the data of State Space from Postgresql to csv."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "output:Heartrate.csv                           \tnumber of stay_id:6669\n",
                  "output:ABPs.csv                                \tnumber of stay_id:2129\n",
                  "output:NBPs.csv                                \tnumber of stay_id:6632\n",
                  "output:ABPd.csv                                \tnumber of stay_id:2131\n",
                  "output:NBPd.csv                                \tnumber of stay_id:6632\n",
                  "output:ABPm.csv                                \tnumber of stay_id:2168\n",
                  "output:NBPm.csv                                \tnumber of stay_id:6632\n",
                  "output:RespiratoryRate.csv                     \tnumber of stay_id:6669\n",
                  "output:TemperatureF.csv                        \tnumber of stay_id:6576\n",
                  "output:TemperatureC.csv                        \tnumber of stay_id:832\n",
                  "output:PH_A.csv                                \tnumber of stay_id:3663\n",
                  "output:PH_V.csv                                \tnumber of stay_id:3184\n",
                  "output:ABE.csv                                 \tnumber of stay_id:3645\n",
                  "output:Hematocrit_serum.csv                    \tnumber of stay_id:6596\n",
                  "output:Hematocrit_wholeblood.csv               \tnumber of stay_id:1200\n",
                  "output:Hemoglobin.csv                          \tnumber of stay_id:6591\n",
                  "output:Platele.csv                             \tnumber of stay_id:6591\n",
                  "output:WBC.csv                                 \tnumber of stay_id:6593\n",
                  "output:Chloride_serum.csv                      \tnumber of stay_id:6618\n",
                  "output:Chloride_wholeblood.csv                 \tnumber of stay_id:1058\n",
                  "output:Calcium_ion.csv                         \tnumber of stay_id:3459\n",
                  "output:Calcium_nonion.csv                      \tnumber of stay_id:6549\n",
                  "output:Potassium_serum.csv                     \tnumber of stay_id:6617\n",
                  "output:Potassium_wholeblood.csv                \tnumber of stay_id:2181\n",
                  "output:Sodium_serum.csv                        \tnumber of stay_id:6617\n",
                  "output:Sodium_wholeblood.csv                   \tnumber of stay_id:1401\n",
                  "output:ProthrombinTime.csv                     \tnumber of stay_id:6011\n",
                  "output:PTT.csv                                 \tnumber of stay_id:5970\n",
                  "output:INR.csv                                 \tnumber of stay_id:6011\n",
                  "output:SaO2.csv                                \tnumber of stay_id:1905\n",
                  "output:SpO2.csv                                \tnumber of stay_id:6665\n",
                  "Error executing SQL statement: Length mismatch: Expected axis has 0 elements, new values have 3 elements\n",
                  "drop:PaO2.csv                                \tnumber of stay_id:0\n",
                  "output:PaCO2.csv                               \tnumber of stay_id:3645\n",
                  "output:FiO2.csv                                \tnumber of stay_id:4044\n",
                  "output:BUN.csv                                 \tnumber of stay_id:6616\n",
                  "output:Creatinine_serum.csv                    \tnumber of stay_id:6618\n",
                  "output:Creatinine_wholeblood.csv               \tnumber of stay_id:40\n",
                  "output:Albumin.csv                             \tnumber of stay_id:3608\n",
                  "output:AnionGap.csv                            \tnumber of stay_id:6617\n",
                  "output:TotalBilirubin.csv                      \tnumber of stay_id:4797\n",
                  "output:DirectBilirubin.csv                     \tnumber of stay_id:818\n",
                  "output:ALT.csv                                 \tnumber of stay_id:4771\n",
                  "output:AST.csv                                 \tnumber of stay_id:4769\n",
                  "drop:UrineOutput.csv                         \tnumber of stay_id:1\n",
                  "output:GCS_EyeOpening.csv                      \tnumber of stay_id:6669\n",
                  "output:GCS_VerbalResponse.csv                  \tnumber of stay_id:6668\n",
                  "output:GCS_MotorResponse.csv                   \tnumber of stay_id:6668\n",
                  "['220045', '220050', '220179', '220051', '220180', '220052', '220181', '220210', '223761', '223762', '223830', '220274', '224828', '220545', '226540', '220228', '227457', '220546', '220602', '226536', '225667', '225625', '227442', '227464', '220645', '226534', '227465', '227466', '227467', '220227', '220277', '220235', '223835', '225624', '220615', '229761', '227456', '227073', '225690', '225651', '220644', '220587', '220739', '223900', '223901']\n"
               ]
            }
         ],
         "source": [
            "threshold = 1000\n",
            "# generate the list of itemid\n",
            "itemid_list_state=[]\n",
            "# generate the dictionary of itemid-abbr\n",
            "with open('csv/itemid_label_state.csv', newline='') as csvfile:\n",
            "    # Create a CSV reader object\n",
            "    reader = csv.reader(csvfile)\n",
            "    # Skip the header row\n",
            "    next(reader)\n",
            "    # Initialize an empty dictionary and list\n",
            "    label_state = {}\n",
            "    itemid_list = []\n",
            "    # Iterate over the rows in the CSV file\n",
            "    for row in reader:\n",
            "        # Add the key-value pair to the dictionary\n",
            "        label_state[row[0]] = row[1]\n",
            "        # Add the itemid to the list\n",
            "        itemid_list.append(row[0])\n",
            "\n",
            "# Execute the SQL command\n",
            "with conn.cursor() as cursor:\n",
            "    \n",
            "    for itemid in itemid_list:\n",
            "        \n",
            "        command_count = \"select count(distinct(stay_id)) from mimiciv_derived.sepsis_state where itemid={};\".format(itemid)\n",
            "        cursor.execute(command_count)\n",
            "        num = cursor.fetchone()[0]\n",
            "        \n",
            "        command = \"select stay_id, charttime, valuenum from mimiciv_derived.sepsis_state where itemid={} order by charttime;\".format(itemid)\n",
            "        cursor.execute(command)   \n",
            "        result = cursor.fetchall()\n",
            "        df=pd.DataFrame(result)\n",
            "        try:\n",
            "            df.columns = ['stay_id', 'charttime', 'valuenum']\n",
            "            df.astype({'stay_id': int, 'charttime': 'datetime64[ns]', 'valuenum': float})\n",
            "        except (Exception, psycopg2.DatabaseError) as error:\n",
            "            print(\"Error executing SQL statement:\", error) \n",
            "        os.makedirs('./output/data/data_raw/state', exist_ok=True)\n",
            "        \n",
            "        if (num<num_stay_ids/threshold):\n",
            "            print(\"drop:{0:40}\".format(label_state[str(itemid)]+\".csv\")+\"\\tnumber of stay_id:\"+str(num))\n",
            "            \n",
            "        else:\n",
            "            df.to_csv('./output/data/data_raw/state/{}.csv'.format(label_state[str(itemid)]),index=0)\n",
            "            itemid_list_state.append(itemid)\n",
            "            print(\"output:{0:40}\".format(label_state[str(itemid)]+\".csv\")+\"\\tnumber of stay_id:\"+str(num))\n",
            "        \n",
            "        \n",
            "        \n",
            "    cursor.close()\n",
            "print(itemid_list_state)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 3.2 Data transfer of Action Space\n",
            "\n",
            "In *3.2.1* we transfer data of following actions from Postgresql to csv:\n",
            "\n",
            " - IV fluid bolus\n",
            "   - NaCl_0.9%\n",
            "   - Dextrose_5%\n",
            " - Vasopressors \n",
            "   - Norepinephrine\n",
            "   - Phenylephrine\n",
            "   - Vasopressin\n",
            "   - Epinephrine\n",
            "   - Dopamine\n",
            "   - Dobutamine\n",
            "   - Milrinone\n",
            "\n",
            "After calculation and comparison, in *3.2.2* we will omit the two insignificant vasopressors: Dobutamine and Milrinone.\n",
            "\n",
            "So that in *3.2.3* we can directly obtain a vasopressors_norepinephrine_equivalent_dose based on \"Vasopressor dose equivalence: A scoping review and suggested formula\" by Goradia et al. 2020."
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 3.2.1 Read action data of IV fluid bolus & Vasopressors"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "output action (IV_fluid_bolus):\tNaCl_0_9%.csv\n",
                  "output action (IV_fluid_bolus):\tDextrose_5%.csv\n",
                  "output action (vasopressors):\tNorepinephrine.csv\n",
                  "output action (vasopressors):\tVasopressin.csv\n",
                  "output action (vasopressors):\tDobutamine.csv\n",
                  "output action (vasopressors):\tMilrinone.csv\n",
                  "output action (vasopressors):\tPhenylephrine.csv\n",
                  "output action (vasopressors):\tDopamine.csv\n",
                  "output action (vasopressors):\tEpinephrine.csv\n"
               ]
            }
         ],
         "source": [
            "# TODO: here the value per minute is computed, do we need this? (paper4.1,P5)\n",
            "\n",
            "# generate the dictionary of action\n",
            "with open('csv/itemid_label_action.csv', newline='') as csvfile:\n",
            "    # Create a CSV reader object\n",
            "    reader = csv.reader(csvfile)\n",
            "    # Skip the header row\n",
            "    next(reader)\n",
            "    # Initialize an empty dictionary and list\n",
            "    action_label = {}\n",
            "    a_itemid_list = []\n",
            "    # Iterate over the rows in the CSV file\n",
            "    for row in reader:\n",
            "        # Add the key-value pair to the dictionary\n",
            "        action_label[row[0]] = row[1]\n",
            "        # Add the itemid to the list\n",
            "        a_itemid_list.append(row[0])\n",
            "\n",
            "if os.path.exists('./output/data/data_raw/action/IV_fluid_bolus'):shutil.rmtree('./output/data/data_raw/action/IV_fluid_bolus')\n",
            "if os.path.exists('./output/data/data_raw/action/vasopressors'):shutil.rmtree('./output/data/data_raw/action/vasopressors')\n",
            "os.makedirs('./output/data/data_raw/action/IV_fluid_bolus')\n",
            "os.makedirs('./output/data/data_raw/action/vasopressors')\n",
            "\n",
            "with conn.cursor() as cursor:\n",
            "\n",
            "    for itemid in a_itemid_list:\n",
            "        # QUESTION: why do we need to order by starttime?\n",
            "        command = \"select stay_id, starttime, endtime, amount from mimiciv_derived.sepsis_action where itemid={} order by starttime;\".format(itemid)\n",
            "        cursor.execute(command)\n",
            "\n",
            "        result = cursor.fetchall()\n",
            "        df = pd.DataFrame(result)\n",
            "        df.columns = ['stay_id', 'starttime', 'endtime', 'amount']\n",
            "        \n",
            "        df['duration'] = df['endtime'] - df['starttime']\n",
            "        df['duration'] = df['duration'].dt.total_seconds()  # Convert duration to seconds\n",
            "        df['duration'] = df['duration'] / 60\n",
            "        df['value_per_minute'] = df['amount'] / df['duration']\n",
            "        \n",
            "        if \"Dextrose_5%\" in action_label[str(itemid)] or \"NaCl_0_9%\" in action_label[str(itemid)]:\n",
            "            df.to_csv('./output/data/data_raw/action/IV_fluid_bolus/{}.csv'.format(action_label[str(itemid)]), index=0)\n",
            "            print(\"output action (IV_fluid_bolus):\\t\"+action_label[str(itemid)]+\".csv\")\n",
            "        else:\n",
            "            df.to_csv('./output/data/data_raw/action/vasopressors/{}.csv'.format(action_label[str(itemid)]), index=0)\n",
            "            print(\"output action (vasopressors):\\t\"+action_label[str(itemid)]+\".csv\")\n",
            "    cursor.close()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 3.2.2 Analyze action data of IV fluid bolus & Vasopressors "
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### 3.2.2.1 Get IV fluid bolus statistical data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "项目\tIV fluid bolus\tCount\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "1\tV_fluid_bolus/Dextrose_5%\t144445\n",
                  "2\tV_fluid_bolus/NaCl_0_9%\t183559\n"
               ]
            }
         ],
         "source": [
            "# 文件夹路径\n",
            "folder_path = 'output/data/data_raw/action/IV_fluid_bolus'\n",
            "print('项目\\tIV fluid bolus\\tCount')\n",
            "\n",
            "# 获取文件夹中的所有CSV文件路径\n",
            "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
            "\n",
            "index = 0\n",
            "# 遍历每个CSV文件，获取行数\n",
            "for file_path in file_paths:\n",
            "    file_name = os.path.basename(file_path)\n",
            "    with open(file_path, 'r', newline='') as csvfile:\n",
            "        csv_reader = csv.reader(csvfile)\n",
            "        rows = sum(1 for row in csv_reader)\n",
            "        index += 1\n",
            "    print(f\"{index}\\t{file_path[29:-4]}\\t{rows}\")"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### 3.2.2.2 Get Vasopressors statistical data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "项目\tVasopressors\tCount\t占比\t前n项占比\n",
                  "1\t/vasopressors/Norepinephrine\t60081\t66.46%\t66.46%\n",
                  "2\t/vasopressors/Phenylephrine\t19667\t21.76%\t88.22%\n",
                  "3\t/vasopressors/Vasopressin\t3763\t4.16%\t92.38%\n",
                  "4\t/vasopressors/Epinephrine\t2829\t3.13%\t95.51%\n",
                  "5\t/vasopressors/Dopamine\t2448\t2.71%\t98.22%\n",
                  "6\t/vasopressors/Dobutamine\t1132\t1.25%\t99.47%\n",
                  "7\t/vasopressors/Milrinone\t481\t0.53%\t100.0%\n",
                  "Total\t\t\t90401\t100%\t100%\n"
               ]
            }
         ],
         "source": [
            "# 文件夹路径\n",
            "folder_path = 'output/data/data_raw/action/vasopressors'\n",
            "print('项目\\tVasopressors\\tCount\\t占比\\t前n项占比')\n",
            "\n",
            "# 获取文件夹中的所有CSV文件路径\n",
            "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
            "\n",
            "# 存储每个CSV文件的行数\n",
            "rows_dict = {}\n",
            "\n",
            "# 计算行数之和\n",
            "total_rows = 0\n",
            "\n",
            "# 遍历每个CSV文件，获取行数\n",
            "for file_path in file_paths:\n",
            "    with open(file_path, 'r', newline='') as csvfile:\n",
            "        csv_reader = csv.reader(csvfile)\n",
            "        rows = sum(1 for row in csv_reader)\n",
            "        rows_dict[file_path] = rows\n",
            "        total_rows += rows\n",
            "\n",
            "# 按行数大小对字典进行排序\n",
            "sorted_rows = sorted(rows_dict.items(), key=lambda x: x[1], reverse=True)\n",
            "\n",
            "total_rows_first_n = 0\n",
            "index = 0\n",
            "# 打印排序结果\n",
            "for file_path, rows in sorted_rows:\n",
            "    index += 1\n",
            "    total_rows_first_n += rows\n",
            "    print(f\"{index}\\t{file_path[27:-4]}\\t{rows}\\t{round(rows/total_rows*100,2)}%\\t{round(total_rows_first_n/total_rows*100,2)}%\")\n",
            "print(f\"Total\\t\\t\\t{total_rows}\\t100%\\t100%\")"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### 3.2.2.3 Conclusion of Analysis of action data\n",
            "\n",
            "前5项血管加压药占比:98.22%，可见前5项血管加压药占比较大，因此我们只需要考虑前5项血管加压药即可。忽略Dobutamine和Milrinone。\n",
            "\n",
            "后两项血管加压药也被此文献忽视。'Vasopressor dose equivalence: A scoping review and suggested formula by Goradia et al. 2020'\n",
            "\n",
            "前5项血管加压药的等效计量值可以直接从mimiciv_derived.norepinephrine_equivalent_dose中获取:"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 3.2.3 Obtain Vasopressor_norepinephrine_equivalent_dose"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "output action (vasopressors_norepinephrine_equivalent_dose.csv)\n",
                  "\n",
                  "项目\tVasopressors_norepinephrine_equivalent_dose\tCount\n",
                  "1\tta_raw/action/vasopressors_norepinephrine_equivalent_dose\t91823\n"
               ]
            }
         ],
         "source": [
            "# 从mimiciv_derived.norepinephrine_equivalent_dose中获取前5项血管加压药的等效计量值 norepinephrine_equivalent_dose_rate\n",
            "\n",
            "with conn.cursor() as cursor:\n",
            "\n",
            "    command = \"select stay_id, starttime, endtime, norepinephrine_equivalent_dose from mimiciv_derived.norepinephrine_equivalent_dose where stay_id in (select stay_id from mimiciv_derived.sepsis_patients_cohort);\" # get norepinephrine_equivalent_dose_rate\n",
            "    cursor.execute(command)\n",
            "\n",
            "    result = cursor.fetchall()\n",
            "    df = pd.DataFrame(result)\n",
            "    df.columns = ['stay_id', 'starttime', 'endtime', 'norepinephrine_equivalent_dose_rate'] # norepinephrine_equivalent_dose in mcg/kg/min\n",
            "    \n",
            "    df['duration'] = df['endtime'] - df['starttime']\n",
            "    df['duration'] = df['duration'].dt.total_seconds()  # Convert duration to seconds\n",
            "    df['duration'] = df['duration'] / 60\n",
            "    df['norepinephrine_equivalent_dose_rate'] = df['norepinephrine_equivalent_dose_rate'].astype(float)\n",
            "    \n",
            "    df.to_csv('./output/data/data_raw/action/vasopressors_norepinephrine_equivalent_dose.csv', index=0)\n",
            "    print(\"output action (vasopressors_norepinephrine_equivalent_dose.csv)\")\n",
            "\n",
            "    cursor.close()\n",
            "\n",
            "# 文件夹路径\n",
            "folder_path = 'output/data/data_raw/action'\n",
            "print('\\n项目\\tVasopressors_norepinephrine_equivalent_dose\\tCount')\n",
            "\n",
            "# 获取文件夹中的所有CSV文件路径\n",
            "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
            "\n",
            "index = 0\n",
            "# 遍历每个CSV文件，获取行数\n",
            "for file_path in file_paths:\n",
            "    file_name = os.path.basename(file_path)\n",
            "    with open(file_path, 'r', newline='') as csvfile:\n",
            "        csv_reader = csv.reader(csvfile)\n",
            "        rows = sum(1 for row in csv_reader)\n",
            "        index += 1\n",
            "    print(f\"{index}\\t{file_path[14:-4]}\\t{rows}\")"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 4.Hourly Sample"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 4.1 Hourly Sample on State Space"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 4.1.1 Define function hourly_sample_state(selected_id)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "k = 5\n",
            "\n",
            "# Create a custom imputation function\n",
            "def lq_distance_imputer(X):\n",
            "    # Calculate the Lq distances\n",
            "    distances = np.abs(X - X[:, -1].reshape(-1, 1)) ** q\n",
            "\n",
            "    # Calculate the weights based on the inverse distances\n",
            "    weights = 1 / distances\n",
            "\n",
            "    # Normalize the weights\n",
            "    weights /= np.sum(weights, axis=1).reshape(-1, 1)\n",
            "\n",
            "    # Multiply the weights with the feature values and compute the weighted mean\n",
            "    return np.sum(weights * X[:, :-1], axis=1)\n",
            "\n",
            "def hourly_sample_state(selected_id):\n",
            "    # Set the folder path where the CSV files are stored\n",
            "    folder_path = './output/data/data_raw/state/'\n",
            "    columns=['chartdatetime']\n",
            "\n",
            "    # define a new dataframe\n",
            "    df_output = pd.DataFrame(columns=columns)\n",
            "\n",
            "\n",
            "    #FIXME: flag only for test\n",
            "    i=0\n",
            "\n",
            "    # Loop through the file paths and read each file into a DataFrame\n",
            "    for itemid in itemid_list_state:\n",
            "        \n",
            "        feature=label_state[str(itemid)]\n",
            "        path = folder_path + feature+'.csv'\n",
            "        \n",
            "        # Load the CSV file into a pandas DataFrame\n",
            "        dtypes={'stay_id':str,'chartdatetime':str,feature:str}\n",
            "        df = pd.read_csv(path,names=['stay_id', 'chartdatetime', feature ],dtype=dtypes)\n",
            "        df['stay_id'] = pd.to_numeric(df['stay_id'], errors='coerce')\n",
            "        \n",
            "        # Filter the DataFrame to only the rows from the selected stay_id\n",
            "        df_filtered = df[df['stay_id'] == selected_id]\n",
            "        \n",
            "        # Convert the 'datetime' column to a datetime object\n",
            "        df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
            "        #df_filtered['chartdatetime'] = df_filtered['chartdatetime'].apply(pd.to_datetime)\n",
            "\n",
            "        # Set the 'datetime' column as the DataFrame's index\n",
            "        df_filtered.set_index('chartdatetime', inplace=True)\n",
            "        \n",
            "        # # Resample the DataFrame hourly and forward fill missing values\n",
            "        df_hourly= df_filtered.resample('H').ffill()\n",
            "        df_hourly=df_hourly.drop(['stay_id'],axis=1)\n",
            "        \n",
            "        df_output['chartdatetime'] = pd.to_datetime(df_output['chartdatetime'])\n",
            "        #df_output = pd.concat([df_output, df_hourly], join=\"outer\",sort=False)\n",
            "        df_output=pd.merge(df_output,df_hourly,how='outer',on='chartdatetime')\n",
            "        \n",
            "        \n",
            "        \n",
            "        #imputer = KNNImputer(n_neighbors=k,weights='distance',missing_values=float('NaN'))\n",
            "        \n",
            "        imputer = KNNImputer(n_neighbors=k,weights=lq_distance_imputer,missing_values=float('NaN'))\n",
            "        imputed_values=imputer.fit_transform(df_output[feature].values.reshape(-1,1))\n",
            "        df_output[feature] = imputed_values\n",
            "        \n",
            "        i+=1\n",
            "        if(i==8): break\n",
            "        \n",
            "    \n",
            "\n",
            "    os.makedirs('./output/data/data_hourly_sample/state', exist_ok=True)\n",
            "    df_output.reset_index().to_csv(f'./output/data/data_hourly_sample/state/stay_id{selected_id}.csv',index=0)\n",
            "\n",
            "    # print(i)\n",
            "    # # Reset the index and save the resampled DataFrame to a new CSV file\n",
            "    # df_hourly.reset_index().to_csv('./output/your_resampled_file.csv', index=False, header=None)  "
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 4.1.2 Loop hourly_resample through stay_ids to obtain all the patient states."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/tmp/ipykernel_63352/3516527021.py:44: SettingWithCopyWarning: \n",
                  "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                  "Try using .loc[row_indexer,col_indexer] = value instead\n",
                  "\n",
                  "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: SettingWithCopyWarning: \n",
                  "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                  "Try using .loc[row_indexer,col_indexer] = value instead\n",
                  "\n",
                  "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: SettingWithCopyWarning: \n",
                  "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                  "Try using .loc[row_indexer,col_indexer] = value instead\n",
                  "\n",
                  "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: SettingWithCopyWarning: \n",
                  "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                  "Try using .loc[row_indexer,col_indexer] = value instead\n",
                  "\n",
                  "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: SettingWithCopyWarning: \n",
                  "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                  "Try using .loc[row_indexer,col_indexer] = value instead\n",
                  "\n",
                  "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: SettingWithCopyWarning: \n",
                  "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                  "Try using .loc[row_indexer,col_indexer] = value instead\n",
                  "\n",
                  "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: SettingWithCopyWarning: \n",
                  "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                  "Try using .loc[row_indexer,col_indexer] = value instead\n",
                  "\n",
                  "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: SettingWithCopyWarning: \n",
                  "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                  "Try using .loc[row_indexer,col_indexer] = value instead\n",
                  "\n",
                  "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n",
                  "/tmp/ipykernel_63352/3516527021.py:44: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
                  "  df_filtered.loc[:,'chartdatetime'] = pd.to_datetime(df_filtered['chartdatetime'].copy())\n"
               ]
            }
         ],
         "source": [
            "selected_id = 32950566\n",
            "# selected_id = 32950566\n",
            "\n",
            "hourly_sample_state(selected_id)\n",
            "# for selected_id in stay_ids:\n",
            "#     hourly_sample_state(selected_id)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 4.2 Hourly Sample on Action Space"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 4.2.1 Hourly sample vasopressors_equivalent_dose for both continuous and discrete action space"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 36,
         "metadata": {},
         "outputs": [],
         "source": [
            "def hourly_sample_vasopressors_equivalent_dose(selected_id): # (mcg/kg/min)\n",
            "    # Read the CSV file and convert the date columns to datetime objects\n",
            "    df = pd.read_csv('./output/data/data_raw/action/vasopressors_norepinephrine_equivalent_dose.csv')\n",
            "    df['starttime'] = pd.to_datetime(df['starttime'])\n",
            "    df['endtime'] = pd.to_datetime(df['endtime'])\n",
            "\n",
            "    # Filter for the selected stay_id\n",
            "    df_filtered = df[df['stay_id'] == selected_id].copy()\n",
            "\n",
            "    # Create a new DataFrame to hold data by minute\n",
            "    df_minutes = []\n",
            "\n",
            "    # For each row, generate data for each minute\n",
            "    for _, row in df_filtered.iterrows():\n",
            "        minutes = int((row['endtime'] - row['starttime']).total_seconds() / 60)\n",
            "        for minute in range(minutes):\n",
            "            time = row['starttime'] + timedelta(minutes=minute)\n",
            "            df_minutes.append({'stay_id': row['stay_id'], \n",
            "                               'starttime': time, \n",
            "                               'endtime': time + timedelta(minutes=1), \n",
            "                               'norepinephrine_equivalent_dose_rate': row['norepinephrine_equivalent_dose_rate'], \n",
            "                               'duration': 1})\n",
            "\n",
            "    # Convert the list to a DataFrame\n",
            "    df_minutes = pd.DataFrame(df_minutes)\n",
            "\n",
            "    # Set starttime as the index\n",
            "    df_minutes.set_index('starttime', inplace=True)\n",
            "\n",
            "    # Resample and get the max value for each hour\n",
            "    df_resampled = df_minutes['norepinephrine_equivalent_dose_rate'].resample('H').max()\n",
            "\n",
            "    # If the norepinephrine_equivalent_dose_rate is NaN, replace it with 0\n",
            "    df_resampled.fillna(0, inplace=True)\n",
            "\n",
            "    # Reset the index\n",
            "    df_resampled = df_resampled.reset_index()\n",
            "\n",
            "    # Discretize norepinephrine_equivalent_dose_rate\n",
            "    bins = [-np.inf, 0, 0.08, 0.22, 0.45, np.inf]\n",
            "    labels = [1, 2, 3, 4, 5]\n",
            "    df_resampled['Discretized_vasopressors'] = pd.cut(df_resampled['norepinephrine_equivalent_dose_rate'], bins=bins, labels=labels)\n",
            "\n",
            "    # Write the discretized DataFrame to a CSV file\n",
            "    os.makedirs('./output/data/data_hourly_sample/action/discrete_resampled_vasopressors_norepinephrine_equivalent_dose', exist_ok=True)\n",
            "    df_resampled.to_csv(f'./output/data/data_hourly_sample/action/discrete_resampled_vasopressors_norepinephrine_equivalent_dose/{selected_id}.csv', index=False)\n",
            "\n",
            "# Use the function\n",
            "hourly_sample_vasopressors_equivalent_dose(31872514)\n",
            "\n",
            "# for selected_id in stay_ids:\n",
            "#     try:\n",
            "#         hourly_sample_action(selected_id)\n",
            "#     except:\n",
            "#         print(f'Error with {selected_id}')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 4.2.2 Hourly sample IV_fluid_bolus for both continuous and discrete action space"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 37,
         "metadata": {},
         "outputs": [],
         "source": [
            "def hourly_sample_IV_fluid_bolus(selected_id): # (mL/1 hour)\n",
            "    # Create a list of two fluid types\n",
            "    fluid_types = ['Dextrose_5%', 'NaCl_0_9%']\n",
            "    \n",
            "    # Initialize an empty DataFrame to store the results\n",
            "    df_resampled_all = pd.DataFrame()\n",
            "\n",
            "    # For each fluid type\n",
            "    for fluid in fluid_types:\n",
            "        # Read the CSV file and convert the date column to datetime objects\n",
            "        df = pd.read_csv(f'./output/data/data_raw/action/IV_fluid_bolus/{fluid}.csv')\n",
            "        df['starttime'] = pd.to_datetime(df['starttime'])\n",
            "        df['endtime'] = pd.to_datetime(df['endtime'])\n",
            "\n",
            "        # Filter the selected stay_id\n",
            "        df_filtered = df[df['stay_id'] == selected_id].copy()\n",
            "\n",
            "        # Create a new DataFrame for minute-wise data\n",
            "        df_minutes = []\n",
            "\n",
            "        # Generate data for each minute for each row\n",
            "        for _, row in df_filtered.iterrows():\n",
            "            minutes = int((row['endtime'] - row['starttime']).total_seconds() / 60)\n",
            "            for minute in range(minutes):\n",
            "                time = row['starttime'] + timedelta(minutes=minute)\n",
            "                df_minutes.append({'stay_id': row['stay_id'], \n",
            "                                   'starttime': time, \n",
            "                                   'endtime': time + timedelta(minutes=1), \n",
            "                                   f'{fluid}_per_hour': row['value_per_minute'], \n",
            "                                   'duration': 1})\n",
            "\n",
            "        # Convert the list to a DataFrame\n",
            "        df_minutes = pd.DataFrame(df_minutes)\n",
            "\n",
            "        # Set starttime as the index\n",
            "        df_minutes.set_index('starttime', inplace=True)\n",
            "\n",
            "        # Resample and calculate the total fluid per hour\n",
            "        df_resampled = df_minutes[f'{fluid}_per_hour'].resample('H').sum()\n",
            "\n",
            "        # Reset the index\n",
            "        df_resampled = df_resampled.reset_index()\n",
            "\n",
            "        # Add the resampled results of this fluid type to the overall result DataFrame\n",
            "        if df_resampled_all.empty:\n",
            "            df_resampled_all = df_resampled\n",
            "        else:\n",
            "            df_resampled_all = pd.merge(df_resampled_all, df_resampled, on='starttime')\n",
            "\n",
            "    # Calculate the sum of the two fluid types per hour\n",
            "    df_resampled_all['IV_fluid_bolus_per_hour'] = df_resampled_all['Dextrose_5%_per_hour'] + df_resampled_all['NaCl_0_9%_per_hour']\n",
            "\n",
            "    # If the amount is missing, replace it with 0\n",
            "    df_resampled_all.fillna(0, inplace=True)\n",
            "\n",
            "    # Discretize norepinephrine_equivalent_dose_rate\n",
            "    bins = [-np.inf, 0, 12.5, 45, 132.5, np.inf]\n",
            "    labels = [1, 2, 3, 4, 5]\n",
            "    df_resampled_all['Discretized_IV_fluid_bolus'] = pd.cut(df_resampled_all['IV_fluid_bolus_per_hour'], bins=bins, labels=labels)\n",
            "\n",
            "    # Write to a CSV file\n",
            "    os.makedirs('./output/data/data_hourly_sample/action/IV_fluid_bolus/', exist_ok=True)\n",
            "    df_resampled_all.to_csv(f'./output/data/data_hourly_sample/action/IV_fluid_bolus/{selected_id}.csv', index=False)\n",
            "\n",
            "# Use the function\n",
            "hourly_sample_IV_fluid_bolus(31872514)\n",
            "\n",
            "# for selected_id in stay_ids:\n",
            "#     try:\n",
            "#         hourly_sample_IV_fluid_bolus(selected_id)\n",
            "#     except:\n",
            "#         print(f'Error with {selected_id}')"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "MLhomework",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.11"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
